{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Workshop\n",
    "\n",
    "## Tutorial I: data queries, dataset catalog creation\n",
    "\n",
    "<div style=\"border-left: 4px solid #0366d6; padding: 0.5em; background-color: #deecfc;\">\n",
    "  ℹ️ If you want to know more about these topics please refer to:\n",
    "  <ul>\n",
    "  <li><code>freva_client</code> library <a href='https://freva-org.github.io/freva-nextgen/index.html#installation-and-configuration' target=\"_blank\">installation</a></li>\n",
    "  <li>Databrowser <a href=\"https://freva-org.github.io/freva-nextgen/databrowser/python-lib.html\">python module </a></li>\n",
    "</ul>  \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, we'll learn how to use the `freva_client` library to explore and access available datasets and at the end customize the dataset on Freva to our own liking.\n",
    "\n",
    "First let's see how to install the freva client library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Client Library\n",
    "\n",
    "| Environment | Installation Command |\n",
    "|-------------|---------------------|\n",
    "| DKRZ/Levante (Recommended) | `$ module load clint gems` |\n",
    "| Conda (Local) | $ `conda create -n freva-client-env -c conda-forge freva-client -y` |\n",
    "| Python (Local) | `$ pip install freva-client` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  border-left: 6px solid rgb(236, 114, 0);\n",
    "  background-color:rgb(253, 231, 157);\n",
    "  color:rgb(19, 19, 18);\n",
    "  padding: 1em;\n",
    "  font-size: 110%;\n",
    "  border-radius: 4px;\n",
    "  margin: 1em 0;\n",
    "\">\n",
    "⚠️ <strong>ATTENTION</strong>: For the Freva Databrowser workshop, please open a Terminal tab in Jupyter and write the following:\n",
    "<pre><code>$ module load clint gems\n",
    "$ da-workshop-setup\n",
    "</code></pre>\n",
    "\n",
    "<br>\n",
    "And then from kernel environment list, please choose, <code>DA Workshop (python)</code>\n",
    "Now your environment is ready to start!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let quickly ckeck if `freva_client` is available on our current kernel environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from freva_client import databrowser, __version__\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we'll run a simple analysis on the [MPI Grand Ensemble data](https://mpimet.mpg.de/en/research/modeling/grand-ensemble), a large collection of climate simulations. Our goal will be to create an ensemble of global averaged time series of 2 m air temperature.\n",
    "\n",
    "The data browser organizes metadata in a **tree-like hierarchy**. At the top of this structure is the **`project`** facet (equivalent to the **`mip-era`** for CMIP6 Data Reference Syntax) and then it goes down as it follows:\n",
    "```\n",
    ".\n",
    "├── project\n",
    "│   ├── product\n",
    "│   │   ├── institute\n",
    "│   │   │   ├── model\n",
    "│   │   │   │   ├── experiment\n",
    "...\n",
    "```\n",
    "These facets are organised as `{key: value}` pairs. First and foremost, let's find out which search keys are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we know that the Grand-Ensemble data is stored under `mpi-ge` but we don't know whether it's under `project` or `product` etc. The databrowser is here to help. You can simply use the `facet` argument to search for all entries containing a certain value, such as `mpi-ge`.\n",
    "\n",
    "Let's get the project(s) of all search keys (or facets) that contain `mpi-ge`. We can use the `metadata_search` function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search(\"mpi-ge\")[\"project\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to create a time series of 2 m air temperature we will need to check whether the `tas` (near-surface air temperature) variable is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"tas\" in databrowser.metadata_search(\"mpi-ge\")[\"variable\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do have a rough overview of the available `tas` data and also future scenarios, that is, timesteps from today until 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(project=\"mpi-ge\", variable=\"tas\", time=\"2025-01 to 2100-12\")\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same vein, let's query the available output time frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.metadata_search(\"mpi-ge\", variable=\"tas\")[\"time_frequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how many files were found we can apply the `len` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_yr =  databrowser(\n",
    "    flavour=\"cmip6\", \n",
    "    project=\"mpi-ge\", \n",
    "    time_frequency=\"mon\", \n",
    "    variable=\"tas\",\n",
    "    time=\"2025-01 to 2100-12\"\n",
    ")\n",
    "len(db_yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the `metadata_search` method we can check the meta data with using `metadata` property. This will give you the metadata search parameters that were used to create the `db` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_yr.metadata[\"experiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `picontrol` experiment is unexpected! Let's check the what is going on. We create a new search and check the files belonging to that search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcontrol = databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=\"picontrol\")\n",
    "len(pcontrol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files we can \"convert\" our search to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(pcontrol)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a reverse search, that is, check what meta-data is associated with a file, for that we use the `file=` parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search(file=files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't want this pre-industrial control run among our selected datasets we will tell the databrowser to ignore it.\n",
    "\n",
    "We can use the `!` to *not* include a certain value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=\"!picontrol\")\n",
    "db.metadata[\"experiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a global time series for each experiment. To do this properly, we need to account for the fact that the grid cells in a latitude-longitude grid do not all represent the same area — they shrink toward the poles. Since `xarray` does not natively apply area weights when computing spatial means, we need to define a helper function that applies cosine-latitude weighting to approximate the true area of each grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "def field_mean(\n",
    "    data: xr.DataArray,\n",
    "    lat_name: str = \"lat\",\n",
    "    lon_name: str = \"lon\",\n",
    "    mean_dims: tuple[str, str] = (\"lat\", \"lon\")\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Compute an area-weighted mean over latitude and longitude using cosine latitude weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xr.DataArray\n",
    "        Input data array with latitude and longitude coordinates.\n",
    "    lat_name : str, optional\n",
    "        Name of the latitude coordinate. Default is \"lat\".\n",
    "    lon_name : str, optional\n",
    "        Name of the longitude coordinate. Default is \"lon\".\n",
    "    mean_dims : Sequence[str], optional\n",
    "        Dimensions over which to compute the mean. Default is (\"lat\", \"lon\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Area-weighted mean of the input data over the specified dimensions.\n",
    "    \"\"\"\n",
    "    # Extract latitude values\n",
    "    lat = data[lat_name]\n",
    "\n",
    "    # Compute cosine of latitude in radians\n",
    "    weights = np.cos(np.deg2rad(lat))\n",
    "\n",
    "    # Normalize weights so they sum to 1 (over just lat)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # Apply weighted mean over specified dimensions\n",
    "    return data.weighted(weights).mean(dim=mean_dims, keep_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the search result of the databrowser object to directly open our dataset in xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = {}\n",
    "\n",
    "for exp in db.metadata[\"experiment\"]:\n",
    "    \n",
    "    ensembles = []\n",
    "\n",
    "    members = sorted(\n",
    "        databrowser.metadata_search(\n",
    "            project=\"mpi-ge\",\n",
    "            variable=\"tas\",\n",
    "            time_frequency=\"mon\",\n",
    "            time=\"2025-01 to 2100-12\",\n",
    "            experiment=exp\n",
    "        )[\"ensemble\"]\n",
    "    )\n",
    "\n",
    "    # Going through all ensemble members might take some time,\n",
    "    # let's make a cut at 5 member x experiment for demo purposes...\n",
    "    for num, member in enumerate(members[:5]):\n",
    "        print(f\"Reading {exp} | Ensemble {member} ({num+1}/5)\", end=\"\\r\")\n",
    "\n",
    "        ds = xr.open_mfdataset(\n",
    "            databrowser(\n",
    "                project=\"mpi-ge\",\n",
    "                variable=\"tas\",\n",
    "                time_frequency=\"mon\",\n",
    "                time=\"2025-01 to 2100-12\",\n",
    "                experiment=exp,\n",
    "                ensemble=member\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add an ensemble and experiment dimension as we combine the data set later\n",
    "        ts = field_mean(ds[\"tas\"]).expand_dims(ensemble=[member], experiment=[exp])\n",
    "        ensembles.append(ts)\n",
    "    print(\"\")\n",
    "\n",
    "    time_series[exp] = xr.concat(ensembles, dim=\"ensemble\", combine_attrs=\"override\")\n",
    "\n",
    "# Combine all experiments\n",
    "data = xr.concat(time_series.values(), dim=\"experiment\", combine_attrs=\"override\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a color cycle for different experiments\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, exp in enumerate(data.experiment.values):\n",
    "    ts = data.sel(experiment=exp).resample(time=\"1YE\").mean()\n",
    "    ts_min = ts.min(dim=\"ensemble\").squeeze()\n",
    "    ts_max = ts.max(dim=\"ensemble\").squeeze()\n",
    "    ts_mean = ts.mean(dim=\"ensemble\").squeeze()\n",
    "    # Plot min–max shading\n",
    "    plt.fill_between(\n",
    "        ts.time,\n",
    "        ts_min,\n",
    "        ts_max,\n",
    "        color=colors[i % len(colors)],\n",
    "        alpha=0.2,\n",
    "        label=None,\n",
    "    )\n",
    "    # Plot mean line\n",
    "    plt.plot(\n",
    "        ts.time,\n",
    "        ts_mean,\n",
    "        color=colors[i % len(colors)],\n",
    "        linewidth=2,\n",
    "        label=f\"{exp}\"\n",
    "    )\n",
    "\n",
    "# Add plot decorations\n",
    "plt.title(\"Ensemble Mean and Spread for Each Experiment\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(f\"{data.attrs['long_name']} [{data.attrs['units']}]\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset catalogs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've already found our target dataset on Freva, we may want to export the full metadata for other project's partner that might not have direct Freva access or for us to download and access it somewhere else on any other HPC system.\n",
    "\n",
    "In this section we are going to introduce two different types of Catalogues: \n",
    "1. The [**intake-esm**](https://intake-esm.readthedocs.io/en/stable/) catalog provides a lightweight, Python-friendly interface to the metadata of large Earth System Model archives. By pointing to a central JSON index, it lets you discover, filter, and load climate model outputs—such as temperature, precipitation, or ocean variables—without downloading entire datasets. The catalog structure follows the CMIP/ESM conventions, enabling easy subsetting by attributes like project name, variable, experiment, and time period. Once exported as a standalone YAML file, your subsetted catalog can be shared with collaborators who can query and load data locally, with no direct access to the original archive required.\n",
    "\n",
    "\n",
    "2. The [**STAC (SpatioTemporal Asset Catalog)**](https://stacspec.org/en) static catalog defines a simple, filesystem-based layout for geospatial metadata. A static catalog bundles Catalog, Collection, and Item JSON files into a set of directories that mirror your data hierarchy, with no dynamic search API. Bundling the entire catalog into a ZIP archive makes it trivial to distribute or archive a snapshot of your dataset inventory—satellite imagery, climate projections, or any spatiotemporal assets—for offline use, disaster recovery, or reproducible analyses. Once unzipped, the folder structure and JSON files provide the same discovery semantics as a live STAC endpoint.  \n",
    "\n",
    "\n",
    "First, we’ll use **intake-esm** to subset by our chosen search keys-value pairs:  \n",
    "- project: `mpi-ge`\n",
    "- variable: `tas`\n",
    "- time_frequency: `mon`\n",
    "- time: `'2025-01 to 2100-12'`\n",
    "- experiment: `picontrol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=\"picontrol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.intake_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then export the catalogue as e.g. JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = db.intake_catalogue()\n",
    "cat.serialize(\n",
    "    name=\"intake_catalog\",\n",
    "    directory=\".\",\n",
    "    catalog_type=\"file\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We’ll now perform the same operation on a **STAC static catalog**: download the entire catalog as a ZIP archive so you can share or inspect it offline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.stac_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To complete our explanation about STAC catalog, the **STAC static catalog** is implemented as a set of flat files on a web server or object store (e.g., S3). It exposes the same Item, Catalog, and Collection JSON structure as a dynamic STAC, but without a `/search` endpoint—making it easy to bundle and distribute as a ZIP for disaster recovery or offline use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA Workshop (python)",
   "language": "python",
   "name": "da-workshop-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
