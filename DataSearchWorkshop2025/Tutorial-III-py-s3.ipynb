{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e3ec15-55f4-4856-ba85-0a9a61e3b3e2",
   "metadata": {},
   "source": [
    "# Data Analysis Workshop\n",
    "\n",
    "## Tutorial III: Accessing Data on S3\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 6px solid rgb(236, 114, 0);\n",
    "  background-color:rgb(253, 231, 157);\n",
    "  color:rgb(19, 19, 18);\n",
    "  padding: 1em;\n",
    "  font-size: 110%;\n",
    "  border-radius: 4px;\n",
    "  margin: 1em 0;\n",
    "\">\n",
    "⚠️ <strong>ATTENTION</strong>: if the kernel is not already set, please change it to the <code>DA Workshop (python)</code>\n",
    "</div>\n",
    "\n",
    "This tutorial is similar to the first one! However we’ll learn how to use `xarray`, `zarr` and `s3fs` to explore datasets available via S3 but also upload results to it!\n",
    "\n",
    "To get started, we’ll run a simple analysis on a remote NCAR dataset (hosted on AWS S3) to plot global monthly means of near-surface temperature, using data from the [CESM2 Large Ensemble (LENS2)](https://www.cesm.ucar.edu/community-projects/lens2).\n",
    "\n",
    "LENS2 is a 100-member CESM2 ensemble (1850–2100) based on CMIP6 historical and SSP370 forcing. It includes two 50-member subsets: one using original biomass burning emissions, and another using a smoothed version (SMBB) to reduce discontinuities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaca030-1b8e-4d40-89f4-5a2674304724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from freva_client import databrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b5376-d627-4f76-aa55-8a94afb698de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "databrowser.metadata_search(fs_type=\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dd22b-22c1-4279-b5cf-c68edd8b4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keys = {\n",
    "    'fs_type': 's3',\n",
    "    'project':'cesm2-le',\n",
    "    'experiment': 'historical',\n",
    "    'realm': 'atm',\n",
    "    'time_frequency':'monthly'\n",
    "}\n",
    "'ts' in databrowser.metadata_search(**search_keys)['variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763ad11-4f73-4e9e-9daa-854021a944df",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(variable='ts', **search_keys)\n",
    "list(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb86eff-b95c-44c3-ad13-1b3d0420c2a1",
   "metadata": {},
   "source": [
    "## Let's quickly check how to open the first dataset with xarray\n",
    "\n",
    "Since all are zarr datasets we can use `xr.open_zarr` or `xr.open_dataset(engine='zarr')`.\n",
    "\n",
    "Because the data is public, we do not need credentials to open the data. We specify `anon` (anonymous) in order for xarray to not sign the requests.\n",
    "\n",
    "If data is **not** public we will get `403 Forbidden`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3566b-8f14-42f3-a153-401e1a6d70ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "xr.open_zarr(list(db)[0], storage_options={ 'anon':True })\n",
    "xr.open_dataset(list(db)[0], engine='zarr', storage_options={ 'anon':True })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fe089-f43e-4705-aa51-6738379377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "def field_mean(\n",
    "    data: xr.DataArray,\n",
    "    lat_name: str = \"lat\",\n",
    "    lon_name: str = \"lon\",\n",
    "    mean_dims: tuple[str, str] = (\"lat\", \"lon\")\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Compute an area-weighted mean over latitude and longitude using cosine latitude weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xr.DataArray\n",
    "        Input data array with latitude and longitude coordinates.\n",
    "    lat_name : str, optional\n",
    "        Name of the latitude coordinate. Default is \"lat\".\n",
    "    lon_name : str, optional\n",
    "        Name of the longitude coordinate. Default is \"lon\".\n",
    "    mean_dims : Sequence[str], optional\n",
    "        Dimensions over which to compute the mean. Default is (\"lat\", \"lon\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Area-weighted mean of the input data over the specified dimensions.\n",
    "    \"\"\"\n",
    "    # Extract latitude values\n",
    "    lat = data[lat_name]\n",
    "\n",
    "    # Compute cosine of latitude in radians\n",
    "    weights = np.cos(np.deg2rad(lat))\n",
    "\n",
    "    # Normalize weights so they sum to 1 (over just lat)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # Apply weighted mean over specified dimensions\n",
    "    return data.weighted(weights).mean(dim=mean_dims, keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c62fea-1104-4388-9483-e14b72c37bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "s3_opts = { 'anon':True }\n",
    "time_series = {}\n",
    "\n",
    "for fileurl in db:\n",
    "    print(f\"Opening {fileurl}...\")\n",
    "    engine = {'engine':'zarr'} if fileurl.endswith('zarr') else {}\n",
    "    ds = xr.open_dataset(fileurl, **engine, storage_options=s3_opts)\n",
    "    ds = ds.rename({'member_id':'ensemble'})\n",
    "    \n",
    "    # Going through all ensembles might take some time\n",
    "    # let's make a cut at 5 member for demo purposes\n",
    "    # Memory usage might spike to ~70GB \n",
    "    members=ds.ensemble[:5]\n",
    "    ds = ds.sel(ensemble=members)    \n",
    "    \n",
    "    dataset_name=Path(fileurl).stem   \n",
    "    \n",
    "    mean_ts = field_mean(ds[\"TS\"])\n",
    "    mean_ts.attrs['source_dataset'] = fileurl\n",
    "    time_series[dataset_name] = mean_ts\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05742a-79c4-40a5-9286-16e31514e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a color cycle for different experiments\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "plot = plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, (exp, data) in enumerate(time_series.items()):\n",
    "    ts = data.resample(time=\"1YE\").mean()\n",
    "    ts_min = ts.min(dim=\"ensemble\").squeeze()\n",
    "    ts_max = ts.max(dim=\"ensemble\").squeeze()\n",
    "    ts_mean = ts.mean(dim=\"ensemble\").squeeze()\n",
    "    time_str = ts.time.dt.strftime(\"%Y\").values\n",
    "    time_values = np.array(ts.time.values, dtype='datetime64[ns]')\n",
    "    # Plot min–max shading\n",
    "    plt.fill_between(\n",
    "        time_values,\n",
    "        ts_min.values,\n",
    "        ts_max.values,\n",
    "        color=colors[i % len(colors)],\n",
    "        alpha=0.2,\n",
    "        label=None,\n",
    "    )\n",
    "    # Plot mean line\n",
    "    plt.plot(\n",
    "        time_values,\n",
    "        ts_mean.values,\n",
    "        color=colors[i % len(colors)],\n",
    "        linewidth=2,\n",
    "        label=f\"{exp}\"\n",
    "    )\n",
    "\n",
    "# Add plot decorations\n",
    "plt.title(\"Ensemble Mean and Spread for Each Dataset\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(f\"{data.attrs['long_name']} [{data.attrs['units']}]\")  # Replace with actual units\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501ed1f-7013-49a3-9198-191b98fdb3f0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Let's save the plot on S3, as well as the datasets!\n",
    "\n",
    "We start by setting up a dictionary with the configuration we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640c59d-5fd1-4dd4-94bd-c04470551e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getuser # to get USER env variable\n",
    "USERNAME = getuser()\n",
    "s3_config = {\n",
    "    'bucket' : 'freva',\n",
    "    'endpoint' :'https://s3.eu-dkrz-1.dkrz.cloud', # DKRZ Minio S3\n",
    "    'prefix' : f'workshop/{USERNAME}', # Avoid users writting object with same prefix\n",
    "    'access_key_id' : \"s3handson\", # Only valid during the workshop\n",
    "    'secret_access_key' : \"s3handson\", # Only valid during the workshop\n",
    "    'region' : 'eu-dkrz-1',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed17f90-3d2f-49f9-9c82-b9fe55396f4f",
   "metadata": {},
   "source": [
    "Now we create a wrapper on S3 that mimics a local filesystem. This will be important for `netcdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6652d6a-0394-4fff-8d3f-7548f2f20103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    key = s3_config['access_key_id'],\n",
    "    secret = s3_config['secret_access_key'],\n",
    "    endpoint_url = s3_config['endpoint'],\n",
    ")\n",
    "\n",
    "full_prefix = s3_config['bucket']+'/'+s3_config['prefix'] \n",
    "testobj = f'{full_prefix}/hi.txt'\n",
    "s3.write_bytes(testobj, b'Hi!\\n')\n",
    "\n",
    "print(f\"Writing data to S3 works! Test it with:\\ncurl {s3_config['endpoint']}/{testobj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd630624-8d48-4fed-91fe-6ef97deb6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://s3.eu-dkrz-1.dkrz.cloud/freva/workshop/${USER}/hi.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d69b7-caef-4239-8b35-da38304ff7d2",
   "metadata": {},
   "source": [
    "### Save the figure on S3\n",
    " - Open the object (\"file\") on S3 where the figure should be saved\n",
    " - Save the figure into that opened object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e58178-1cf1-4339-bf39-c140e32978bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_path = f'{full_prefix}/figure-ts-mean.png'\n",
    "with s3.open(figure_path, 'wb') as f:\n",
    "    plot.savefig(f)\n",
    "print(f\"Open this link to get the image: {s3_config['endpoint']}/{figure_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac904f5d-80e7-476e-bdd2-5a61b3889712",
   "metadata": {},
   "source": [
    "### Let's now write the data to S3 as both NetCDF and ZARR\n",
    "\n",
    "Note that since we are writing to an object store, we cannot use `xr.to_netcdf` with an open file object — `seek` operations are not supported in this context!\n",
    "\n",
    "Instead, simply save the file locally first, then copy it to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78586e-cc61-406e-b267-7d5cd7e43e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_netcdf(s3_path, dataset:xr.Dataset):\n",
    "    tmp_name=f\"/scratch/{USERNAME[0]}/{USERNAME}/{name}.nc\"\n",
    "    dataset.to_netcdf(tmp_name, engine='h5netcdf')\n",
    "        \n",
    "    ## Copy to s3\n",
    "    with s3.open(s3_path,'wb') as s3file:\n",
    "        with open(tmp_name, 'rb') as tmpf:\n",
    "            s3file.write(tmpf.read())\n",
    "\n",
    "    ## Check if we can open with Xarray!\n",
    "    with s3.open(s3_path,'rb') as s3file:\n",
    "        xr.open_dataset(s3file, engine='h5netcdf')\n",
    "    \n",
    "    ## We can now remove the local copy\n",
    "    import os\n",
    "    os.remove(tmp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd8dad-6627-4611-af63-f959ef18dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_zarr(store, dataset: xr.Dataset):     \n",
    "    import zarr\n",
    "    zarr.config.set(default_zarr_format=2)\n",
    "    \n",
    "    import numcodecs\n",
    "    codec = numcodecs.Blosc(shuffle=1, clevel=6)\n",
    "    \n",
    "    data_encoding = {}\n",
    "    for var in dataset.variables:\n",
    "        data_encoding[var] = { \"compressors\": codec}\n",
    "\n",
    "    dataset.to_zarr(store,\n",
    "                    mode='w', # OVERWRITES existing data!!\n",
    "                    encoding=data_encoding,\n",
    "                    consolidated=True, # consolidate metadata for fast access\n",
    "                    storage_options= { # We cannot use the anonymous mode anymore\n",
    "                        'key':s3_config['access_key_id'], \n",
    "                        'secret':s3_config['secret_access_key'],\n",
    "                        'endpoint_url': s3_config['endpoint'],\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d1b4e-72e7-45a4-8e4b-3a75e70416a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset_to_s3(name:str, dataset:xr.Dataset, file_format='nc'):\n",
    "    _supported_types = ('nc','zarr')\n",
    "    file_format = file_format.replace('.','')\n",
    "    if file_format not in _supported_types:\n",
    "        raise Exception('Unsuported file format, use one of')\n",
    "    \n",
    "    s3_path = f'{full_prefix}/{name}.{file_format}'\n",
    "    if file_format == 'nc':\n",
    "        write_netcdf(s3_path, dataset)\n",
    "\n",
    "    elif file_format == 'zarr':\n",
    "        write_zarr(f's3://{s3_path}', dataset)\n",
    "\n",
    "    netcdf_mode= 'zarr' if file_format == 'zarr' else 'bytes'\n",
    "    print(f\"Try running:\\n\\t\" \\\n",
    "          f\"/fastdata/freva/opt/bin/ncdump -h {s3_config['endpoint']}/{s3_path}#mode=s3,{netcdf_mode}\\n\\t\"\\\n",
    "          f\"/fastdata/freva/opt/bin/cdo sinfo {s3_config['endpoint']}/{s3_path}#mode=s3,{netcdf_mode}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46981bf5-14b0-44c1-b634-98c88d0c01f4",
   "metadata": {},
   "source": [
    "We save the mean temperature array in S3 as both Zarr and NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088b2d7-5daa-409f-8111-301968630051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, data in time_series.items():\n",
    "    ## for CDO it is important that time is the first dimension\n",
    "    ## also it cannot have indexers of type string\n",
    "    ensemble = data.ensemble.values\n",
    "    ensemble_id = np.arange(len(ensemble))\n",
    "    \n",
    "    dataset = data.to_dataset() \\\n",
    "            .transpose('time',...) \\\n",
    "            .assign_coords(ensemble_id=('ensemble',ensemble_id)) \\\n",
    "            .swap_dims({'ensemble':'ensemble_id'}) \\\n",
    "            .reset_coords(drop=True)\\\n",
    "            .assign_attrs(ensembles= ', '.join(data.ensemble.values))\n",
    "    \n",
    "    write_dataset_to_s3(name, dataset, file_format='nc')\n",
    "    write_dataset_to_s3(name, dataset, file_format='zarr')\n",
    "\n",
    "public_url = f\"https://eu-dkrz-1.dkrz.cloud/browser/{s3_config['bucket']}/{s3_config['prefix']}\"\n",
    "f\"{s3_config['endpoint']}/browser/{s3_config['bucket']}/{s3_config['prefix']}\"\n",
    "print(f\"Browser the data in {public_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa37b7-95e1-4b36-a240-0e4ca33f939a",
   "metadata": {},
   "source": [
    "A bespoke compiled binary of cdo and ncdump will allow us to check the content of these S3 files:\n",
    "\n",
    "1. for NetCDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cb078-3805-4088-ae70-5efdfa9a92ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/fastdata/freva/opt/bin/ncdump -h https://s3.eu-dkrz-1.dkrz.cloud/freva/workshop/$USER/cesm2LE-historical-smbb-TS.nc#mode=s3,bytes\n",
    "!/fastdata/freva/opt/bin/cdo sinfo https://s3.eu-dkrz-1.dkrz.cloud/freva/workshop/$USER/cesm2LE-historical-smbb-TS.nc#mode=s3,bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4b894-2cee-4bd2-b39d-95d148ac04e5",
   "metadata": {},
   "source": [
    "2. for Zarr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b2779-b91b-491b-967c-7cbe7b68f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/fastdata/freva/opt/bin/ncdump -h https://s3.eu-dkrz-1.dkrz.cloud/freva/workshop/$USER/cesm2LE-historical-smbb-TS.zarr#mode=s3,zarr\n",
    "!/fastdata/freva/opt/bin/cdo sinfo https://s3.eu-dkrz-1.dkrz.cloud/freva/workshop/$USER/cesm2LE-historical-smbb-TS.zarr#mode=s3,zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294fac5-1a38-4bf0-b8d9-a5e54177d3c9",
   "metadata": {},
   "source": [
    "We can also test whether the s3 files can be opened with xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Opening\")\n",
    "for name in time_series.keys():\n",
    "    file = f'https://s3.eu-dkrz-1.dkrz.cloud/freva/workshop/{USERNAME}/{name}'\n",
    "    print(f\"{file}.nc\\n{file}.zarr\")\n",
    "    xr.open_dataset(f'{file}.nc', engine='h5netcdf')\n",
    "    xr.open_zarr(f'{file}.zarr')\n",
    "\n",
    "print('All output datasets opened!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA Workshop (python)",
   "language": "python",
   "name": "da-workshop-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
