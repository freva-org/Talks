{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330bda16-9e10-44e5-aaa3-f519d91a5462",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1339d0a6-72a8-45cc-805f-8d5aa98f2f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: EVALUATION_SYSTEM_CONFIG_DIR=/work/ch1187/clint/nextgems/freva/\n",
      "env: EVALUATION_SYSTEM_CONFIG_FILE=/work/ch1187/clint/nextgems/freva/evaluation_system.conf\n"
     ]
    }
   ],
   "source": [
    "%env EVALUATION_SYSTEM_CONFIG_DIR=/work/ch1187/clint/nextgems/freva/\n",
    "%env EVALUATION_SYSTEM_CONFIG_FILE=/work/ch1187/clint/nextgems/freva/evaluation_system.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaca030-1b8e-4d40-89f4-5a2674304724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from freva_client import databrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b5376-d627-4f76-aa55-8a94afb698de",
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search(fs_type=\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70da200-6aaa-45a3-9a1d-35148841d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search(fs_type=\"s3\")['project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9facd997-7069-40e4-b695-002de3964df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for facet, values in databrowser.metadata_search(fs_type='s3').items():\n",
    "    if facet in ('realm','time_frequency','experiment'):\n",
    "        print(facet,'\\n\\t',values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dd22b-22c1-4279-b5cf-c68edd8b4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keys = {\n",
    "    'fs_type': 's3',\n",
    "    'project':'cesm2-le',\n",
    "    'experiment': 'historical',\n",
    "    'realm': 'atm',\n",
    "    'time_frequency':'monthly'\n",
    "}\n",
    "'ts' in databrowser.metadata_search(**search_keys)['variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763ad11-4f73-4e9e-9daa-854021a944df",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(variable='ts', **search_keys)\n",
    "db,list(db)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fe089-f43e-4705-aa51-6738379377e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "def field_mean(\n",
    "    data: xr.DataArray,\n",
    "    lat_name: str = \"lat\",\n",
    "    lon_name: str = \"lon\",\n",
    "    mean_dims: tuple[str, str] = (\"lat\", \"lon\")\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Compute an area-weighted mean over latitude and longitude using cosine latitude weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xr.DataArray\n",
    "        Input data array with latitude and longitude coordinates.\n",
    "    lat_name : str, optional\n",
    "        Name of the latitude coordinate. Default is \"lat\".\n",
    "    lon_name : str, optional\n",
    "        Name of the longitude coordinate. Default is \"lon\".\n",
    "    mean_dims : Sequence[str], optional\n",
    "        Dimensions over which to compute the mean. Default is (\"lat\", \"lon\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Area-weighted mean of the input data over the specified dimensions.\n",
    "    \"\"\"\n",
    "    # Extract latitude values\n",
    "    lat = data[lat_name]\n",
    "\n",
    "    # Compute cosine of latitude in radians\n",
    "    weights = np.cos(np.deg2rad(lat))\n",
    "\n",
    "    # Normalize weights so they sum to 1 (over just lat)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # Apply weighted mean over specified dimensions\n",
    "    return data.weighted(weights).mean(dim=mean_dims, keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3566b-8f14-42f3-a153-401e1a6d70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_zarr(list(db)[0], storage_options={ 'anon':True })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c62fea-1104-4388-9483-e14b72c37bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_opts = { 'anon':True }\n",
    "from pathlib import Path\n",
    "time_series = {}\n",
    "\n",
    "for fileurl in db:\n",
    "    print(f\"Opening {fileurl}...\")\n",
    "    engine = {'engine':'zarr'} if fileurl.endswith('zarr') else {}\n",
    "    %time ds = xr.open_dataset(fileurl, **engine, storage_options=s3_opts)\n",
    "    ds = ds.rename({'member_id':'ensemble'})\n",
    "    ds\n",
    "    \n",
    "    # Going through all ensembles might take some time\n",
    "    # let's make a cut at 5 member for demo purposes\n",
    "    # Memory usage might spike to ~70GB \n",
    "    members=ds.ensemble[:5]\n",
    "    ds = ds.sel(ensemble=members)    \n",
    "    \n",
    "    dataset_name=Path(fileurl).stem   \n",
    "\n",
    "    \n",
    "    mean_ts = field_mean(ds[\"TS\"])\n",
    "    mean_ts.attrs['source_dataset'] = fileurl\n",
    "    time_series[dataset_name] = mean_ts\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cff607-7d85-45b6-844c-046e1ce091f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a color cycle for different experiments\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, (exp, data) in enumerate(time_series.items()):\n",
    "    ts = data.resample(time=\"1YE\").mean()\n",
    "    ts_min = ts.min(dim=\"ensemble\").squeeze()\n",
    "    ts_max = ts.max(dim=\"ensemble\").squeeze()\n",
    "    ts_mean = ts.mean(dim=\"ensemble\").squeeze()\n",
    "    time_str = ts.time.dt.strftime(\"%Y\").values\n",
    "    time_values = np.array(ts.time.values, dtype='datetime64[ns]')\n",
    "    # Plot minâ€“max shading\n",
    "    plt.fill_between(\n",
    "        time_values,\n",
    "        ts_min.values,\n",
    "        ts_max.values,\n",
    "        color=colors[i % len(colors)],\n",
    "        alpha=0.2,\n",
    "        label=None,\n",
    "    )\n",
    "    # Plot mean line\n",
    "    plt.plot(\n",
    "        time_values,\n",
    "        ts_mean.values,\n",
    "        color=colors[i % len(colors)],\n",
    "        linewidth=2,\n",
    "        label=f\"{exp}\"\n",
    "    )\n",
    "\n",
    "# Add plot decorations\n",
    "plt.title(\"Ensemble Mean and Spread for Each Dataset\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(f\"{data.attrs['long_name']} [{data.attrs['units']}]\")  # Replace with actual units\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# We need to save the figure before showing\n",
    "import io\n",
    "fig_buf = io.BytesIO()\n",
    "fig_format = 'png'\n",
    "plt.savefig(fig_buf, format=fig_format)\n",
    "\n",
    "# Finally plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501ed1f-7013-49a3-9198-191b98fdb3f0",
   "metadata": {},
   "source": [
    "# Let's save the plot on S3, as well as the datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4513216-1efc-4cac-bc53-ab48dda36a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ # to get USER env variable\n",
    "USERNAME=environ['USER']\n",
    "s3_config = {\n",
    "    'bucket' : 'freva',\n",
    "    'endpoint' :'https://s3.eu-dkrz-1.dkrz.cloud', # DKRZ Minio S3\n",
    "    'prefix' : f'workshop/{environ['USER']}', # Avoid users writting object with same prefix\n",
    "    'access_key_id' : \"s3handson\", # Only valid during the workshop\n",
    "    'secret_access_key' : \"s3handson\", # Only valid during the workshop\n",
    "    'region' : 'eu-dkrz-1',\n",
    "}\n",
    "\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    key=s3_config['access_key_id'],\n",
    "    secret= s3_config['secret_access_key'],\n",
    "    client_kwargs={'endpoint_url': s3_config['endpoint']},\n",
    ")\n",
    "\n",
    "full_prefix = s3_config['bucket']+'/'+s3_config['prefix'] \n",
    "testobj=f'{full_prefix}/hi.txt'\n",
    "s3.write_bytes(testobj, b'Hi!\\n')\n",
    "\n",
    "print(f\"Writing data to S3 works! Test it with:\\ncurl {s3_config['endpoint']}/{testobj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd630624-8d48-4fed-91fe-6ef97deb6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://s3.eu-dkrz-1.dkrz.cloud/freva/workshop/${USER}/hi.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d69b7-caef-4239-8b35-da38304ff7d2",
   "metadata": {},
   "source": [
    "## Save the figure on S3\n",
    " - Save the figure into a buffer instead of a file\n",
    " - Make a `PUT` request to write the _in-memory_ image into an S3 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bb5cd-2721-4055-8dcf-dae330598dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_path = f'{full_prefix}/figure-ts-mean.{fig_format}'\n",
    "fig_buf.seek(0) ## rewind the buffer to the beginning\n",
    "s3.write_bytes(figure_path, fig_buf.getvalue())\n",
    "print(f\"{s3_config['endpoint']}/{figure_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac904f5d-80e7-476e-bdd2-5a61b3889712",
   "metadata": {},
   "source": [
    "### Lets write the data to S3 as both NetCDF and ZARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d1b4e-72e7-45a4-8e4b-3a75e70416a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset_to_s3(name, dataset:xr.Dataset, file_format='nc'):\n",
    "    _supported_types = ('nc','zarr')\n",
    "    file_format = file_format.replace('.','')\n",
    "    if file_format not in _supported_types:\n",
    "        raise Exception('Unsuported file format, use one of')\n",
    "    \n",
    "    ## Copy to s3\n",
    "    s3_path = f'{full_prefix}/{name}.{file_format}'\n",
    "    if file_format == 'nc':\n",
    "        tmp_name=f\"/scratch/{USERNAME[0]}/{USERNAME}/{name}.nc\"\n",
    "\n",
    "        dataset.to_netcdf(tmp_name, engine='h5netcdf')\n",
    "            \n",
    "        ## Copy to s3\n",
    "        with s3.open(s3_path,'wb') as s3file:\n",
    "            with open(tmp_name, 'rb') as tmpf:\n",
    "                s3file.write(tmpf.read())\n",
    "\n",
    "        ## Check if we can open with Xarray and CDO!\n",
    "        with s3.open(s3_path,'rb') as s3file:\n",
    "            xr.open_dataset(s3file, engine='h5netcdf')\n",
    "\n",
    "        print(f\"Try running:\\n\\t\" \\\n",
    "              f\"/fastdata/k20200/k202186/public/bin/ncdump -h {s3_config['endpoint']}/{s3_path}#mode=s3,bytes\\n\\t\"\\\n",
    "              f\"/fastdata/k20200/k202186/public/bin/cdo sinfo {s3_config['endpoint']}/{s3_path}#mode=s3,bytes\")\n",
    "        \n",
    "        ## We can now remove the local copy\n",
    "        import os\n",
    "        os.remove(tmp_name)\n",
    "\n",
    "    elif file_format == 'zarr':\n",
    "        # We need to create a S3 store zarr store\n",
    "        storage_options = { 'key':s3_config['access_key_id'],'secret':s3_config['secret_access_key'],'client_kwargs':{'endpoint_url': s3_config['endpoint']}}\n",
    "        write_zarr(f's3://{s3_path}', dataset, s3_options=storage_options)\n",
    "        print(f\"Try running:\\n\\t\" \\\n",
    "              f\"/fastdata/k20200/k202186/public/bin/ncdump -h {s3_config['endpoint']}/{s3_path}#mode=s3,zarr\\n\\t\"\\\n",
    "              f\"/fastdata/k20200/k202186/public/bin/cdo sinfo {s3_config['endpoint']}/{s3_path}#mode=s3,zarr\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd8dad-6627-4611-af63-f959ef18dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_zarr(store, data: xr.Dataset, s3_options={}):\n",
    "    if type(data) == xr.DataArray:\n",
    "        data= data.to_dataset()\n",
    "    import zarr\n",
    "    zarr.config.set(default_zarr_format=2)\n",
    "    chunk_encoding = { \"chunk_key_encoding\": zarr.core.chunk_key_encodings.V2ChunkKeyEncoding(separator=\"/\").to_dict() }\n",
    "    \n",
    "    def get_encoding(data: xr.Dataset):\n",
    "        import numcodecs\n",
    "        codec = numcodecs.Blosc(shuffle=1, clevel=6)\n",
    "        return {\n",
    "            var: {\n",
    "                #\"chunks\": get_chunks(dataset[var].dims),\n",
    "                \"compressors\": codec,\n",
    "                \"chunk_key_encoding\": zarr.core.chunk_key_encodings.V2ChunkKeyEncoding(separator=\"/\").to_dict(),\n",
    "            }\n",
    "            for var in data.variables\n",
    "            #if var not in dataset.dims\n",
    "        }\n",
    "    zstore = data.to_zarr(store,\n",
    "                          mode='w', # OVERWRITES!!\n",
    "                          encoding=get_encoding(data),\n",
    "                          consolidated=True, # consolidate metadata for fast access\n",
    "                          storage_options=s3_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088b2d7-5daa-409f-8111-301968630051",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in time_series.items():\n",
    "    ## for CDO it is important that time is the first dimension\n",
    "    ## also it cannot have indexers of type string\n",
    "    ensemble = data.ensemble.values\n",
    "    ensemble_id = np.arange(len(ensemble))\n",
    "    \n",
    "    da = data.to_dataset() \\\n",
    "            .transpose('time',...) \\\n",
    "            .assign_coords(ensemble_id=('ensemble',ensemble_id)) \\\n",
    "            .swap_dims({'ensemble':'ensemble_id'}) \\\n",
    "            .reset_coords(drop=True)\\\n",
    "            .assign_attrs(ensembles= ', '.join(data.ensemble.values))\n",
    "    \n",
    "    write_dataset_to_s3(name, da, file_format='nc')\n",
    "    write_dataset_to_s3(name, da, file_format='zarr')\n",
    "    #write_zarr('/scratch/k/k202186/test.zarr',da)\n",
    "da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freva",
   "language": "python",
   "name": "freva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
