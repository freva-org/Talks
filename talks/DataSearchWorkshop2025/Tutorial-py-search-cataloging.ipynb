{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Workshop\n",
    "\n",
    "## Tutorial I: Search and query, accessing and Cataloging the dataset\n",
    "\n",
    "In this tutorial, we’ll learn how to use the `freva-client` library to explore and access available datasets and at the end customize the dataset on Freva based on own desire.\n",
    "To get started, we’ll run a simple analysis on the [MPI Grand Ensemble data](https://mpimet.mpg.de/en/research/modeling/grand-ensemble), a large collection of climate simulations.\n",
    "The data browser organizes metadata in a **tree-like hierarchy**. At the top of this structure is the **`project`** level.\n",
    "\n",
    "First let's setup and learn about authentication and then keep up with the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Client Library\n",
    "\n",
    "| Environment | Installation Command |\n",
    "|-------------|---------------------|\n",
    "| DKRZ/Levante (Recommended) | `$ module load clint gems`(Terminal) <br> `In [1]: !module load clint gems`(Jupyterhub) |\n",
    "| Conda (Local) | $ `conda create -n freva-client-env -c conda-forge freva-client -y` |\n",
    "| Python (Local) | `$ pip install freva-client` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION**: For the Freva Databrowser workshop, please open the new Jupyter as Terminal and write the following:\n",
    "```bash\n",
    "$ module load clint gems\n",
    "$ da-workshop-setup\n",
    "```\n",
    "And then from kernel environment list, please choose, `DA Workshop (python)`\n",
    "Now your environment is ready to start!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let quickly ckeck if `freva-client` is available on our current kernel environemnt or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2508.0.0\n"
     ]
    }
   ],
   "source": [
    "from freva_client import databrowser, __version__\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, let's findout which search keys are avaiable on the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we know that the Grand-Ensemble data is stored under `mpi-ge` but we don't know whether it's under `project` or `product` etc. The databrowser is here to help. You can simply use the `facet` argument to search for all entries containing a certain value, such as `mpi-ge`. Let's get the project(s) of all search keys (or facets) that contain `mpi-ge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search(\"mpi-ge\")[\"project\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toturial, we are going to create a time series of 2 m air temperature.\n",
    "\n",
    "To do so we have to check if the `tas` variable is available. We can use the `metadata_search` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"tas\" in databrowser.metadata_search(\"mpi-ge\")[\"variable\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's query the available ouput time frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search(\"mpi-ge\", variable=\"tas\")[\"time_frequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do have a rough overview of the available data, to access the data create a so called `instance` of the databrowser class. We want to cover future scenarios, that is timesteps from today until 2100.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\")\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how many files were found we can apply the `len` function to our instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the `metadata_search` method we can check the meta data with using `metadata` property. This will give you the metdata search parameters that were used to create the `db` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.metadata[\"experiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `picontrol` experiment is unexpceted. Let's check the what is going on. We create a new search and check the files belonging to that search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcontrol = databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=\"picontrol\")\n",
    "len(pcontrol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files we can \"convert\" our search to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(pcontrol)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a reverse search, that is check what meta data is assocaited with a file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databrowser.metadata_search(file=files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't want this pre-industrial control run in our databrowser search we tell the databrowser to not use it. We can use the `!` to *not* include a certain value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=\"!picontrol\")\n",
    "db.metadata[\"experiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to create a global time series for each of the experiments. Since we are going to use `xarray` and xarray doesn't support area weights out of box we have to create a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "def field_mean(\n",
    "    data: xr.DataArray,\n",
    "    lat_name: str = \"lat\",\n",
    "    lon_name: str = \"lon\",\n",
    "    mean_dims: tuple[str, str] = (\"lat\", \"lon\")\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Compute an area-weighted mean over latitude and longitude using cosine latitude weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xr.DataArray\n",
    "        Input data array with latitude and longitude coordinates.\n",
    "    lat_name : str, optional\n",
    "        Name of the latitude coordinate. Default is \"lat\".\n",
    "    lon_name : str, optional\n",
    "        Name of the longitude coordinate. Default is \"lon\".\n",
    "    mean_dims : Sequence[str], optional\n",
    "        Dimensions over which to compute the mean. Default is (\"lat\", \"lon\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Area-weighted mean of the input data over the specified dimensions.\n",
    "    \"\"\"\n",
    "    # Extract latitude values\n",
    "    lat = data[lat_name]\n",
    "\n",
    "    # Compute cosine of latitude in radians\n",
    "    weights = np.cos(np.deg2rad(lat))\n",
    "\n",
    "    # Normalize weights so they sum to 1 (over just lat)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # Apply weighted mean over specified dimensions\n",
    "    return data.weighted(weights).mean(dim=mean_dims, keep_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the search result of the databrowser object to directly open dataset in xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = {}\n",
    "for exp in db.metadata[\"experiment\"]:\n",
    "    ensembles = []\n",
    "    # Go throuhg all ensembles, this might take some time (let's make a cut at 5 member for demo purposes)\n",
    "    member = sorted(databrowser.metadata_search(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=exp)[\"ensemble\"])\n",
    "    for num, member in enumerate(member):\n",
    "        print(f\"Reading data and calculating TS for experiment {exp} in ens: {member}\", end=\"\\r\")\n",
    "        ds = xr.open_mfdataset(databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=exp, ensemble=member))\n",
    "        # Add an ensemble and experiment dimension as we combine the data set later\n",
    "        ts = field_mean(ds[\"tas\"]).expand_dims(ensemble=[member], experiment=[exp])\n",
    "        ensembles.append(ts)\n",
    "        if num == 4:\n",
    "            break\n",
    "    time_series[exp] = xr.concat(ensembles, dim=\"ensemble\", combine_attrs=\"override\")\n",
    "# Combine the experiments into one dataset\n",
    "data = xr.concat(time_series.values(), dim=\"experiment\", combine_attrs=\"override\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a color cycle for different experiments\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, exp in enumerate(data.experiment.values):\n",
    "    ts = data.sel(experiment=exp).resample(time=\"1YE\").mean()\n",
    "    ts_min = ts.min(dim=\"ensemble\").squeeze()\n",
    "    ts_max = ts.max(dim=\"ensemble\").squeeze()\n",
    "    ts_mean = ts.mean(dim=\"ensemble\").squeeze()\n",
    "    # Plot min–max shading\n",
    "    plt.fill_between(\n",
    "        ts.time,\n",
    "        ts_min,\n",
    "        ts_max,\n",
    "        color=colors[i % len(colors)],\n",
    "        alpha=0.2,\n",
    "        label=None,\n",
    "    )\n",
    "    # Plot mean line\n",
    "    plt.plot(\n",
    "        ts.time,\n",
    "        ts_mean,\n",
    "        color=colors[i % len(colors)],\n",
    "        linewidth=2,\n",
    "        label=f\"{exp}\"\n",
    "    )\n",
    "\n",
    "# Add plot decorations\n",
    "plt.title(\"Ensemble Mean and Spread for Each Experiment\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(f\"{data.attrs['long_name']} [{data.attrs['units']}]\")  # Replace with actual units\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cataloging Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve already found your target dataset on Freva, you may want to export the full metadata for project's partner who don’t have direct Freva access or you want to download it and access somewhere else on any other HPC system.\n",
    "\n",
    "In this section we are going to introduce two different types of Cataloues: \n",
    "1. The **intake-esm** catalog provides a lightweight, Python-friendly interface to the metadata of large Earth System Model archives. By pointing to a central JSON index, it lets you discover, filter, and load climate model outputs—such as temperature, precipitation, or ocean variables—without downloading entire datasets. The catalog structure follows the CMIP/ESM conventions, enabling easy subsetting by attributes like project name, variable, experiment, and time period. Once exported as a standalone YAML file, your subsetted catalog can be shared with collaborators who can query and load data locally, with no direct access to the original archive required.\n",
    "\n",
    "\n",
    "2. The **STAC (SpatioTemporal Asset Catalog)** static catalog defines a simple, filesystem-based layout for geospatial metadata. A static catalog bundles Catalog, Collection, and Item JSON files into a set of directories that mirror your data hierarchy, with no dynamic search API. Bundling the entire catalog into a ZIP archive makes it trivial to distribute or archive a snapshot of your dataset inventory—satellite imagery, climate projections, or any spatiotemporal assets—for offline use, disaster recovery, or reproducible analyses. Once unzipped, the folder structure and JSON files provide the same discovery semantics as a live STAC endpoint.  \n",
    "\n",
    "\n",
    "First, we’ll use [intake-esm](https://intake-esm.readthedocs.io/en/stable/) to:\n",
    "\n",
    "Subset by our chosen search keys:  \n",
    "- project: `mpi-ge`\n",
    "- time_frequency: `mon`\n",
    "- variable: `tas`\n",
    "- time: `'2025-01 to 2100-12'`\n",
    "- experiment: `picontrol`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = databrowser(project=\"mpi-ge\", variable=\"tas\", time_frequency=\"mon\", time=\"2025-01 to 2100-12\", experiment=\"picontrol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.intake_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll now perform the same operation on a STAC static catalog: download the entire catalog as a ZIP archive so you can share or inspect it offline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.stac_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our explanation about STAC catalog, the **STAC static catalog** is implemented as a set of flat files on a web server or object store (e.g., S3). It exposes the same Item, Catalog, and Collection JSON structure as a dynamic STAC, but without a `/search` endpoint—making it easy to bundle and distribute as a ZIP for disaster recovery or offline use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA Workshop (python)",
   "language": "python",
   "name": "da-workshop-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
